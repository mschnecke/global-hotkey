//! Gemini API client implementation

use base64::Engine;
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::time::Duration;

use super::provider::{AiProvider, AiResponse};
use crate::error::AppError;

const GEMINI_API_BASE: &str = "https://generativelanguage.googleapis.com/v1beta";
const DEFAULT_MODEL: &str = "gemini-2.5-flash-lite";
const MAX_RETRIES: u32 = 3;
const RETRY_DELAY_MS: u64 = 1000;

pub struct GeminiProvider {
    client: Client,
    api_key: String,
    model: String,
}

impl GeminiProvider {
    pub fn new(api_key: String, model: Option<String>) -> Self {
        Self {
            client: Client::new(),
            api_key,
            model: model.unwrap_or_else(|| DEFAULT_MODEL.to_string()),
        }
    }

    fn endpoint(&self) -> String {
        format!(
            "{}/models/{}:generateContent?key={}",
            GEMINI_API_BASE, self.model, self.api_key
        )
    }

    /// Check if an error is retryable (rate limit or overload)
    fn is_retryable_error(status: reqwest::StatusCode, error_message: &str) -> bool {
        let error_lower = error_message.to_lowercase();
        status.as_u16() == 429
            || status.as_u16() == 503
            || error_lower.contains("overloaded")
            || error_lower.contains("too many requests")
            || error_lower.contains("rate limit")
    }

    /// Extract text from Gemini response with proper error handling
    fn extract_response_text(response: GeminiResponse) -> Result<String, AppError> {
        if let Some(error) = response.error {
            return Err(AppError::Ai(error.message));
        }

        let text = response
            .candidates
            .and_then(|c| c.into_iter().next())
            .and_then(|c| c.content.parts.into_iter().next())
            .and_then(|p| p.text)
            .ok_or_else(|| AppError::Ai("No response generated by AI".to_string()))?;

        if text.trim().is_empty() {
            return Err(AppError::Ai("AI returned an empty response".to_string()));
        }

        Ok(text)
    }
}

// Request/Response types
#[derive(Serialize)]
struct GeminiRequest {
    contents: Vec<Content>,
    #[serde(rename = "generationConfig", skip_serializing_if = "Option::is_none")]
    generation_config: Option<GenerationConfig>,
}

#[derive(Serialize)]
struct Content {
    parts: Vec<Part>,
}

#[derive(Serialize)]
#[serde(untagged)]
enum Part {
    Text { text: String },
    InlineData { inline_data: InlineData },
}

#[derive(Serialize)]
struct InlineData {
    mime_type: String,
    data: String,
}

#[derive(Serialize)]
struct GenerationConfig {
    temperature: f32,
    #[serde(rename = "maxOutputTokens")]
    max_output_tokens: u32,
}

#[derive(Deserialize)]
struct GeminiResponse {
    candidates: Option<Vec<Candidate>>,
    error: Option<GeminiError>,
}

#[derive(Deserialize)]
struct Candidate {
    content: CandidateContent,
    #[allow(dead_code)]
    #[serde(rename = "finishReason")]
    finish_reason: Option<String>,
}

#[derive(Deserialize)]
struct CandidateContent {
    parts: Vec<ResponsePart>,
}

#[derive(Deserialize)]
struct ResponsePart {
    text: Option<String>,
}

#[derive(Deserialize)]
struct GeminiError {
    message: String,
}

impl AiProvider for GeminiProvider {
    async fn send_text(
        &self,
        system_prompt: &str,
        user_input: &str,
    ) -> Result<AiResponse, AppError> {
        let combined_prompt = format!("{}\n\n{}", system_prompt, user_input);

        let request = GeminiRequest {
            contents: vec![Content {
                parts: vec![Part::Text {
                    text: combined_prompt,
                }],
            }],
            generation_config: Some(GenerationConfig {
                temperature: 0.1,
                max_output_tokens: 8192,
            }),
        };

        let mut last_error = None;

        for attempt in 0..MAX_RETRIES {
            let response = match self.client.post(self.endpoint()).json(&request).send().await {
                Ok(resp) => resp,
                Err(e) => {
                    last_error = Some(AppError::Ai(format!("Request failed: {}", e)));
                    if attempt < MAX_RETRIES - 1 {
                        tokio::time::sleep(Duration::from_millis(
                            RETRY_DELAY_MS * (attempt as u64 + 1),
                        ))
                        .await;
                        continue;
                    }
                    break;
                }
            };

            let status = response.status();

            // Try to get response body for error checking
            let response_text = response
                .text()
                .await
                .unwrap_or_else(|_| String::from("{}"));

            // Check for retryable errors based on status code
            if Self::is_retryable_error(status, &response_text) {
                let error_type = if status.as_u16() == 429 {
                    "Rate limit exceeded"
                } else {
                    "Model overloaded"
                };
                eprintln!(
                    "⚠️  {} (attempt {}/{}), retrying...",
                    error_type,
                    attempt + 1,
                    MAX_RETRIES
                );
                last_error = Some(AppError::Ai(format!("{}: {}", error_type, response_text)));

                if attempt < MAX_RETRIES - 1 {
                    tokio::time::sleep(Duration::from_millis(
                        RETRY_DELAY_MS * (attempt as u64 + 1),
                    ))
                    .await;
                    continue;
                }
                break;
            }

            // Parse the response
            let gemini_response: GeminiResponse = serde_json::from_str(&response_text)
                .map_err(|e| AppError::Ai(format!("Failed to parse response: {}", e)))?;

            let text = Self::extract_response_text(gemini_response)?;
            return Ok(AiResponse { text });
        }

        Err(last_error.unwrap_or_else(|| AppError::Ai("Request failed after retries".to_string())))
    }

    async fn send_audio(
        &self,
        system_prompt: &str,
        audio_data: &[u8],
        mime_type: &str,
    ) -> Result<AiResponse, AppError> {
        let audio_base64 = base64::engine::general_purpose::STANDARD.encode(audio_data);

        let request = GeminiRequest {
            contents: vec![Content {
                parts: vec![
                    Part::Text {
                        text: system_prompt.to_string(),
                    },
                    Part::InlineData {
                        inline_data: InlineData {
                            mime_type: mime_type.to_string(),
                            data: audio_base64,
                        },
                    },
                ],
            }],
            generation_config: Some(GenerationConfig {
                temperature: 0.1,
                max_output_tokens: 8192,
            }),
        };

        let mut last_error = None;

        for attempt in 0..MAX_RETRIES {
            let response = match self.client.post(self.endpoint()).json(&request).send().await {
                Ok(resp) => resp,
                Err(e) => {
                    last_error = Some(AppError::Ai(format!("Request failed: {}", e)));
                    if attempt < MAX_RETRIES - 1 {
                        tokio::time::sleep(Duration::from_millis(
                            RETRY_DELAY_MS * (attempt as u64 + 1),
                        ))
                        .await;
                        continue;
                    }
                    break;
                }
            };

            let status = response.status();

            // Try to get response body for error checking
            let response_text = response
                .text()
                .await
                .unwrap_or_else(|_| String::from("{}"));

            // Check for retryable errors based on status code
            if Self::is_retryable_error(status, &response_text) {
                let error_type = if status.as_u16() == 429 {
                    "Rate limit exceeded"
                } else {
                    "Model overloaded"
                };
                eprintln!(
                    "⚠️  {} (attempt {}/{}), retrying...",
                    error_type,
                    attempt + 1,
                    MAX_RETRIES
                );
                last_error = Some(AppError::Ai(format!("{}: {}", error_type, response_text)));

                if attempt < MAX_RETRIES - 1 {
                    tokio::time::sleep(Duration::from_millis(
                        RETRY_DELAY_MS * (attempt as u64 + 1),
                    ))
                    .await;
                    continue;
                }
                break;
            }

            // Parse the response
            let gemini_response: GeminiResponse = serde_json::from_str(&response_text)
                .map_err(|e| AppError::Ai(format!("Failed to parse response: {}", e)))?;

            let text = Self::extract_response_text(gemini_response)?;
            return Ok(AiResponse { text });
        }

        Err(last_error.unwrap_or_else(|| AppError::Ai("Request failed after retries".to_string())))
    }

    async fn test_connection(&self) -> Result<bool, AppError> {
        let result = self.send_text("Respond with only: OK", "Test").await;
        match result {
            Ok(_) => Ok(true),
            Err(e) => Err(e),
        }
    }
}
